{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe0a61e3",
   "metadata": {},
   "source": [
    "# ============================ WHY I'M KEEPING THIS ====================================\n",
    "# You've already gotten rid of using this method below, but you might need a part of it \n",
    "# when you implement the next step in your methodology (where you break into durations\n",
    "# based on storms)\n",
    "# ======================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "583c4f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# # filename should be type string ex. 'precipitation.csv' and file path should be in relation to where this file is stored\n",
    "# def TStoStorms(filename, dryTimeBetweenStorms):\n",
    "#     '''accepts a precipitation time series and number of dry time steps before a storm ends and splits it into individual storms'''\n",
    "    \n",
    "#     # read data from csv\n",
    "#     headers = ['datetime', 'pr']\n",
    "#     dtypes = {'col1': 'str', 'col2': 'float'}\n",
    "#     date_cols = ['datetime']\n",
    "#     storms_data = pd.read_csv(filename, dtype = dtypes, header=None, names = headers, parse_dates = date_cols)\n",
    "\n",
    "#     storms_data.loc[ : , 'pr'] = storms_data.loc[ : , 'pr'] / 25.4\n",
    "\n",
    "\n",
    "#     dry = dryTimeBetweenStorms # counter to iterate only on dry days\n",
    "\n",
    "#     # establish big list (storms) which will contain all storms, separated\n",
    "#     # establish little list (current_storm) which will hold precipitation for one storm until it goes to big list\n",
    "#     storms = []\n",
    "#     current_storm = []\n",
    "#     storms.append(current_storm)\n",
    "\n",
    "#     for value in storms_data.loc[ : , 'pr']:\n",
    "#         dry += 1 # count the number of consecutive dry steps\n",
    "\n",
    "\n",
    "#         # if precipitation is higher than a certain threshold, reset dry counter\n",
    "#         if value >= 0.01: \n",
    "#             dry = 0\n",
    "\n",
    "#         # when during a storm, append the current precip value to small list, which is continuously updating big list simultaneously\n",
    "#         if dry < dryTimeBetweenStorms: \n",
    "#             current_storm.append(value)\n",
    "\n",
    "#         # when the dry time reaches the threshold to end a storm, begin a new small list that is being appended to big list\n",
    "#         elif dry == dryTimeBetweenStorms: \n",
    "#             current_storm = []\n",
    "#             storms.append(current_storm)\n",
    "\n",
    "#     #     print(dry, value) # by using this, you can see that it does take need dryTimeBetweenStorms time steps to all be zero before moving to the next storm, it just doesn't count them all in that storm's list entry\n",
    "\n",
    "#     return(storms)\n",
    "    \n",
    "    \n",
    "# TStoStorms('sample_precip1.csv', 5);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21091df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================ WHY I'M KEEPING THIS ====================================\n",
    "# Within the loop at the end, there's some helpful pandas code structure for mutating\n",
    "# dataframes in place.\n",
    "# ======================================================================================\n",
    "\n",
    "\n",
    "# import math\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# def TStoIDF(past_filename, future_filename):\n",
    "#     '''This reads in data from a csv, assigns variable types, calculates CDF shape factors, then solves for past and future precip values.\n",
    "#        Their ratio is the delta factor'''\n",
    "    \n",
    "#     # read data from CSVs\n",
    "#     headers = ['datetime', 'pr']\n",
    "#     dtypes = {'col1': 'str', 'col2': 'float'}\n",
    "#     date_cols = ['datetime']\n",
    "#     past_storms_data = pd.read_csv(past_filename, dtype = dtypes, header=None, names = headers, parse_dates = date_cols)\n",
    "#     future_storms_data = pd.read_csv(future_filename, dtype = dtypes, header=None, names = headers, parse_dates = date_cols)\n",
    "    \n",
    "#     # convert units\n",
    "# #     storms_data.loc[ : , 'pr'] = storms_data.loc[ : , 'pr'] / 25.4\n",
    "    \n",
    "#     # calculate statistics based on full precip inputs\n",
    "#     past_mean_pr = np.mean(past_storms_data.loc[ : , 'pr'])\n",
    "#     past_std_pr = np.std(past_storms_data.loc[ : , 'pr'])\n",
    "    \n",
    "#     future_mean_pr = np.mean(future_storms_data.loc[ : , 'pr'])\n",
    "#     future_std_pr = np.std(future_storms_data.loc[ : , 'pr'])    \n",
    "    \n",
    "#     # calculate shape factors from full input\n",
    "#     ##### these are the equations used\n",
    "#     ### original\n",
    "#     #     mean = mu + beta*gamma\n",
    "#     #     std = pi*beta/sqrt(6)   \n",
    "#     ### solved for shape factors \n",
    "#     #     mu = mean - (beta*gamma)\n",
    "#     #     beta = std*sqrt(6)/pi \n",
    "#     euler_mascheroni_constant = 0.57721\n",
    "    \n",
    "#     past_beta = past_std_pr * math.sqrt(6) / math.pi\n",
    "#     past_mu = past_mean_pr - past_beta * euler_mascheroni_constant\n",
    "\n",
    "#     future_beta = future_std_pr * math.sqrt(6) / math.pi\n",
    "#     future_mu = future_mean_pr - future_beta * euler_mascheroni_constant\n",
    "    \n",
    "  \n",
    "    \n",
    "#     # calculate CDF value for each row (unordered)\n",
    "#     projected_storms_data = past_storms_data\n",
    "#     for count, value in enumerate(projected_storms_data.loc[ : ,'pr']):\n",
    "#         projected_storms_data.loc[count, 'CDF'] = math.exp(-1 * math.exp(-1 * (value - past_mu) / past_beta))\n",
    "#         projected_storms_data.loc[count, 'Projected pr'] = -1 * future_beta * (math.log(-1 * math.log(projected_storms_data.loc[count, 'CDF']))) + future_mu\n",
    "#         projected_storms_data.loc[count, 'Delta'] = projected_storms_data.loc[count, 'Projected pr'] / projected_storms_data.loc[count, 'pr']\n",
    "#     print(projected_storms_data)\n",
    "    \n",
    "#     # to do: change this to run not on every line of precipitation, but instead run for each storm? I think?\n",
    "#     # so it'd be like the total depth of a 24-hr storm was 2 inches in the past, but then it gets projected into the future and would have\n",
    "#     # an equivalent depth of 2.5 inches, so the delta factor would be 2.5 / 2.0 = 1.25 for this one 24-hr storm. Would also need to find the \n",
    "#     # return period for this storm based on the historical 24-hr depth. This way, a regression can be used to find a line for each delta\n",
    "#     # vs. return period for this duration\n",
    "    \n",
    "    \n",
    "# TStoIDF('sample_past_precip.csv', 'sample_future_precip.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060bf2f6",
   "metadata": {},
   "source": [
    "Goal:\n",
    "- read in model data\n",
    "- parse the data, separating it into storms\n",
    "- bin storms by their durations\n",
    "- find deltas based on binned storms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ab2bc04",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'sample_past_precip.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 20\u001b[0m\n\u001b[0;32m     16\u001b[0m     TS \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(TS\u001b[38;5;241m.\u001b[39mtotal_seconds() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m60\u001b[39m)\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m storms_data\n\u001b[1;32m---> 20\u001b[0m storms_data \u001b[38;5;241m=\u001b[39m \u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msample_past_precip.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# storms_data\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[9], line 8\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m      6\u001b[0m dtypes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcol1\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcol2\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[0;32m      7\u001b[0m date_cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 8\u001b[0m storms_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdate_cols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# perform unit conversions\u001b[39;00m\n\u001b[0;32m     11\u001b[0m storms_data\u001b[38;5;241m.\u001b[39mloc[ : , \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m storms_data\u001b[38;5;241m.\u001b[39mloc[ : , \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpr\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\akk34\\miniforge3\\envs\\IDF\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\akk34\\miniforge3\\envs\\IDF\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\akk34\\miniforge3\\envs\\IDF\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\akk34\\miniforge3\\envs\\IDF\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\akk34\\miniforge3\\envs\\IDF\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sample_past_precip.csv'"
     ]
    }
   ],
   "source": [
    "def read_csv(filename):\n",
    "    '''Read in data from csv, assign headers, set datatypes, perform unit conversions if necessary, and find time step for\n",
    "       the dataset in minutes.'''\n",
    "# read data from csv\n",
    "    headers = ['datetime', 'pr']\n",
    "    dtypes = {'col1': 'str', 'col2': 'float'}\n",
    "    date_cols = ['datetime']\n",
    "    storms_data = pd.read_csv(filename, dtype = dtypes, header=None, names = headers, parse_dates = date_cols)\n",
    "    \n",
    "    # perform unit conversions\n",
    "    storms_data.loc[ : , 'pr'] = storms_data.loc[ : , 'pr']\n",
    "\n",
    "    # determine time step by subtracting one datetime from another\n",
    "    TS = storms_data.loc[1, 'datetime'] - storms_data.loc[0, 'datetime']\n",
    "    # convert time delta type to int type in minutes\n",
    "    TS = int(TS.total_seconds() / 60)\n",
    "    \n",
    "    return storms_data\n",
    "\n",
    "storms_data = read_csv('sample_past_precip.csv')\n",
    "# storms_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdcede31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import packaging\n",
    "import matplotlib.pyplot as plt\n",
    "import netCDF4\n",
    "import math\n",
    "\n",
    "def read_netcdf4(filename):\n",
    "    '''Read in data from netcdf4, assign headers, set datatypes, perform unit conversions if necessary'''\n",
    "    # read in netcdf4 data to xarray\n",
    "    storms_data = xr.open_dataset(filename)\n",
    "    # convert xarray dataarray to pandas dataframe\n",
    "    storms_data = storms_data.RAINRATE.to_dataframe()\n",
    "    # get rid of lat long columns that could stay without causing any issues, but I spent ten minutes trying to get it to work, \n",
    "    # so they will remain removed\n",
    "    storms_data = storms_data.drop(['lat', 'lon'], axis=1)\n",
    "    # default index is multiindex. drop the additonal levels\n",
    "    storms_data = storms_data.droplevel(['x', 'y'])\n",
    "    # datetime was being used as the index, not one of the columns. Change it into a column and reset index\n",
    "    storms_data['datetime'] = storms_data.index\n",
    "    storms_data = storms_data.reset_index(drop = True)\n",
    "    # convert datetime column from string to datetime\n",
    "    storms_data['datetime']= pd.to_datetime(storms_data['datetime'])\n",
    "    # rename columns\n",
    "    storms_data = storms_data.rename(columns={\"RAINRATE\": \"pr\"})\n",
    "    # reorder columns\n",
    "    storms_data = storms_data.iloc[:,[1, 0]]\n",
    "\n",
    "#     # perform unit conversions if necessary\n",
    "#     # default units for netcdf4 are mm to 0.1 mm accuracy\n",
    "#     storms_data.loc[ : , 'pr'] = storms_data.loc[ : , 'pr']\n",
    "\n",
    "    return storms_data\n",
    "\n",
    "storms_data = read_netcdf4('CONUS404-19791001-20221001-Dover-Air-Force-Base.nc')\n",
    "\n",
    "display(storms_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4482cb4",
   "metadata": {},
   "source": [
    "This cell is used only while I don't have the full future CONUS404 dataset. It splits\n",
    "the past data in half and pretends that the latter part is from the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ea51a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting dataframe by row index\n",
    "past_storms_data = storms_data.iloc[:188471,:]\n",
    "future_storms_data = storms_data.iloc[188471:,:]\n",
    "future_storms_data.reset_index(inplace = True, drop = True)\n",
    "# print(past_storms_data = df.iloc[:188471,:]\n",
    "\n",
    "print(\"Shape of new dataframes - {} , {}\".format(past_storms_data.shape, future_storms_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8566712b",
   "metadata": {},
   "source": [
    "Finds the timestep (in minutes) of the given data by subtracting two datetimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7491aa7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_timestep(storms_data):\n",
    "    '''Find time step of dataset in minutes'''\n",
    "    # determine time step by subtracting one datetime from another\n",
    "    TS = storms_data.iloc[1, 0] - storms_data.iloc[0, 0]\n",
    "    # convert timedelta type to int type in minutes\n",
    "    TS = TS.total_seconds() / 60\n",
    "    return TS\n",
    "\n",
    "past_TS = find_timestep(past_storms_data)\n",
    "past_TS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddc356c",
   "metadata": {},
   "source": [
    "Test out using different values in the storm durations list and see how that impacts the results.\n",
    "ex: most of the storms were 6hrs in length, so maybe try three durations, <6 hrs, 6 hrs, and >6 hrs\n",
    "\n",
    "Note: .copy() creates a copy of something that is not linked to the original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba72703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create set that contains all the storm durations being used in minutes\n",
    "stormDurations = [60, 120, 180, 360, 720, 1440] # durations every storm will be grouped into (minutes)\n",
    "\n",
    "def separate_into_durations(storms_data,stormDurations, TS):\n",
    "    '''Read in precip data. Put it into rolling sums for each storm and grab the highest rainfall sum for each storm duration\n",
    "       so you can use these max values to calculate the CDF shape parameters.'''\n",
    "   \n",
    "    rolling_avgs_all_durations = []\n",
    "    full_rolling = pd.DataFrame()\n",
    "    for duration in stormDurations:\n",
    "        timesteps = int(duration / TS)\n",
    "\n",
    "        # initialize empty list\n",
    "        rolling_list = []\n",
    "        # loop through precip tables to compute rolling sums and grab datetimes\n",
    "        for count, value in enumerate(storms_data.loc[ : ,'pr']):\n",
    "            # will grab the start time for each rolling sum and store in a list\n",
    "            rolling_start_datetime = storms_data.loc[: , 'datetime']\n",
    "            # will do a rolling sum through a series\n",
    "            rolling_sum = storms_data.iloc[count:timesteps + count, 1].sum(axis=0)\n",
    "            # will compile a list of the rolling sums\n",
    "            rolling_list.append(rolling_sum)     \n",
    "\n",
    "        # create df from the list containing the rolling sums\n",
    "        df_list = pd.DataFrame(rolling_list)\n",
    "        # add column with current duration\n",
    "        df_list['duration'] = duration\n",
    "        \n",
    "        # combine the two dataframes (starting datetime and rolling sum) to get desired output for this duration\n",
    "        rolling_avgs = pd.concat([rolling_start_datetime, df_list], axis=1)\n",
    "        \n",
    "        # store this df so it's not overwritten next loop. This is a bad way of doing this computationally, so fix if \n",
    "        # optimization is needed\n",
    "        full_rolling = pd.concat([full_rolling, rolling_avgs], ignore_index = True)\n",
    "        \n",
    "    # rename columns\n",
    "    full_rolling = full_rolling.rename(columns = {'datetime': 'starting datetime', 0: 'pr_sum'})\n",
    "    \n",
    "    # find max value of pr_sum for each year. This doesn't specify which year it is, but that shouldn't matter\n",
    "    max = full_rolling.groupby(['duration', lambda x: full_rolling['starting datetime'][x].year], as_index = False)[\"pr_sum\"].max()\n",
    "    return max\n",
    "\n",
    "past_max_each_year = separate_into_durations(past_storms_data, stormDurations, past_TS)\n",
    "display(past_max_each_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc22969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run these two functions\n",
    "\n",
    "future_TS = find_timestep(future_storms_data)\n",
    "future_max_each_year = separate_into_durations(future_storms_data, stormDurations, future_TS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7723b0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_past_max_each_year = pd.DataFrame(columns = ['duration', 'pr_sum'], index = list(range(len(past_max_each_year.index))))\n",
    "for count in past_max_each_year.index:\n",
    "    copy_past_max_each_year['duration'].iloc[count] = past_max_each_year['duration'].iloc[count]\n",
    "    copy_past_max_each_year['pr_sum'].iloc[count] = past_max_each_year['pr_sum'].iloc[count]\n",
    "copy_past_max_each_year = copy_past_max_each_year.apply(pd.to_numeric)\n",
    "copy_past_max_each_year\n",
    "\n",
    "copy_future_max_each_year = pd.DataFrame(columns = ['duration', 'pr_sum'], index = list(range(len(future_max_each_year.index))))\n",
    "for count in future_max_each_year.index:\n",
    "    copy_future_max_each_year['duration'].iloc[count] = future_max_each_year['duration'].iloc[count]\n",
    "    copy_future_max_each_year['pr_sum'].iloc[count] = future_max_each_year['pr_sum'].iloc[count]\n",
    "copy_future_max_each_year = copy_future_max_each_year.apply(pd.to_numeric)\n",
    "copy_future_max_each_year\n",
    "# display(copy_future_max_each_year)\n",
    "\n",
    "def compute_CDF(past_max_each_year, future_max_each_year):\n",
    "    '''Calculate CDF shape parameters and deltas for each duration'''\n",
    "#     display(future_max_each_year)\n",
    "#     display(past_max_each_year)\n",
    "\n",
    "    # calculate statistics for each storm duration based on full rolling input of ~past~ data\n",
    "    past_mean_pr = past_max_each_year.groupby(['duration']).mean()\n",
    "    past_std_pr  = past_max_each_year.groupby(['duration']).std()\n",
    "    euler_mascheroni_constant = 0.57721566490153286060651209008240243104215933593992\n",
    "\n",
    "    # calculate shape factors from statistics for each duration\n",
    "    past_beta = past_std_pr * math.sqrt(6) / math.pi\n",
    "    past_mu = past_mean_pr - past_beta * euler_mascheroni_constant\n",
    "    \n",
    "    # calculate statistics for each storm duration based on full rolling input of ~future~ data\n",
    "    future_mean_pr = future_max_each_year.groupby(['duration']).mean()\n",
    "    future_std_pr  = future_max_each_year.groupby(['duration']).std()\n",
    "    euler_mascheroni_constant = 0.57721566490153286060651209008240243104215933593992\n",
    "#     print(future_mean_pr, future_std_pr)\n",
    "\n",
    "    # calculate shape factors from statistics for each duration\n",
    "    future_beta = future_std_pr * math.sqrt(6) / math.pi\n",
    "    future_mu = future_mean_pr - future_beta * euler_mascheroni_constant\n",
    "#     print(future_beta, future_mu)\n",
    "#     print(\"This is beta\", beta)\n",
    "#     print(mu)\n",
    "#     print(beta.iloc[1])\n",
    "\n",
    "    # these are the equations used\n",
    "#     mean = mu + beta*gamma\n",
    "#     std = pi*beta/sqrt(6) \n",
    "#\n",
    "#     Rearranged:\n",
    "#     mu = mean - (beta*gamma)\n",
    "#     beta = std*sqrt(6)/pi  \n",
    "\n",
    "# ***************************************** here is where you find the deltas ********************************************\n",
    "\n",
    "#     past_max_each_year['CDF'] = 0\n",
    "# #     past_max_each_year.drop(['past_CDF'], axis = 1)\n",
    "# #     print(past_max_each_year)\n",
    "#     for count, value in enumerate(past_max_each_year['pr_sum']):\n",
    "# #         print(value)\n",
    "# # #         print(past_max_each_year['duration'].iloc[count])\n",
    "#         past_max_each_year['CDF'].iloc[count] = math.exp(-1 * math.exp(-1 * (value - mu.loc[past_max_each_year['duration'].iloc[count]]) / beta.loc[past_max_each_year['duration'].iloc[count]]))\n",
    "# #         print(beta.loc[past_max_each_year['duration'].iloc[count]])\n",
    "# #         print(mu.loc[past_max_each_year['duration'].iloc[count]])\n",
    "            \n",
    "#     print(beta)\n",
    "    # calculate CDF value for each row (unordered)\n",
    "#     projected_storms_data = past_storms_data\n",
    "    for count, value in enumerate(past_max_each_year['pr_sum']):\n",
    "#         print(beta.loc[past_max_each_year['duration'].iloc[count]])\n",
    "        past_max_each_year.loc[count, 'CDF'] = math.exp(-1 * math.exp(-1 * (value - past_mu.loc[past_max_each_year['duration'].iloc[count]]) / past_beta.loc[past_max_each_year['duration'].iloc[count]]))\n",
    "        past_max_each_year.loc[count, 'projected_pr'] = float(-1 * future_beta.loc[past_max_each_year['duration'].iloc[count]] * (math.log(-1 * math.log(past_max_each_year.loc[count, 'CDF']))) + future_mu.loc[past_max_each_year['duration'].iloc[count]])\n",
    "#         print(past_max_each_year.loc[count, 'CDF'])\n",
    "#         past_max_each_year.loc[count, 'projected_pr'] = future_beta.loc[past_max_each_year['duration'].iloc[count]]\n",
    "#         print(-1 * future_beta.loc[past_max_each_year['duration'].iloc[count]] * (math.log(-1 * math.log(past_max_each_year.loc[count, 'CDF']))) + future_mu.loc[past_max_each_year['duration'].iloc[count]])\n",
    "#         print(future_beta.loc[past_max_each_year['duration'].iloc[count]], future_mu.loc[past_max_each_year['duration'].iloc[count]], past_max_each_year.loc[count, 'CDF'])\n",
    "    \n",
    "        past_max_each_year.loc[count, 'delta'] = past_max_each_year.loc[count, 'projected_pr'] / past_max_each_year.loc[count, 'pr_sum']\n",
    "#     print(past_max_each_year)\n",
    "\n",
    "#     print(beta.loc[60])\n",
    "#     print(beta)\n",
    "#     past_max_each_year\n",
    "    \n",
    "    return past_max_each_year\n",
    "\n",
    "deltas = compute_CDF(copy_past_max_each_year, copy_future_max_each_year) #!!!!!!!!!!!!!!!!!!!!!!!!!!change this!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "display(deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1022f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def read_historical_csv(filename):\n",
    "    '''Read observed historical precipitation time series from .csv'''\n",
    "    \n",
    "    # read data from csv\n",
    "    headers = ['datetime', 'pr']\n",
    "    dtypes = {'col1': 'str', 'col2': 'float'}\n",
    "    date_cols = ['datetime']\n",
    "    historical = pd.read_csv(filename, dtype = dtypes, header=None, names = headers, parse_dates = date_cols)\n",
    "    \n",
    "    # convert units from mm to mm/hr by dividing by 5-min TS\n",
    "    historical.pr = historical.pr * (60 / hist_TS)\n",
    "\n",
    "    return historical\n",
    "historical = 0\n",
    "historical = read_historical_csv('2012_historical.csv')\n",
    "# historical = read_historical_csv('sample_past_precip.csv')\n",
    "# print(historical)\n",
    "\n",
    "hist_TS = find_timestep(historical)\n",
    "print(hist_TS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fb15e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dry_time_between_storms = 360 # number of minutes of dry time before the end of a storm\n",
    "\n",
    "def discretize_by_storm(historical, dry_time_between_storms, TS):\n",
    "    '''Read in historical precipitation TS and assign numbers to each storm that occurs in a new column.'''\n",
    "    \n",
    "    storm_no = 1 # set starting point for labelling the number of storms in your time series\n",
    "    dry_counter = 0 # intialize counter that counts number of consecutive dry time steps\n",
    "    historical['storm_no'] = \"There is nothing here.\" # create new empty column\n",
    "    \n",
    "    for count, precip in enumerate(historical['pr']):\n",
    "        dry_counter += 1\n",
    "        if precip > 0:\n",
    "            dry_counter = 0\n",
    "\n",
    "        if dry_counter == dry_time_between_storms / hist_TS:\n",
    "            storm_no += 1\n",
    "        historical['storm_no'].iloc[count] = storm_no\n",
    "#         print(precip, storm_no)\n",
    "        \n",
    "    return historical\n",
    "discretized_historical = 0\n",
    "# discretized_historical = discretize_by_storm(historical, dry_time_between_storms, hist_TS)\n",
    "# display(discretized_historical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7351ff37",
   "metadata": {},
   "source": [
    "Discretize model storms !! experimental work !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd0d6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "discretized_modeled_past = 0\n",
    "discretized_modeled_future = 0\n",
    "dry_time_between_storms = 360\n",
    "modeled_TS = find_timestep(past_storms_data)\n",
    "discretized_modeled_past = discretize_by_storm(past_storms_data)\n",
    "discretized_modeled_future = discretize_by_storm(future_storms_data)\n",
    "\n",
    "trimmed_discretized_modeled_past = trim_to_storms(discretized_modeled_past, dry_time_between_storms, modeled_TS)\n",
    "trimmed_discretized_modeled_future = trim_to_storms(discretized_modeled_future, dry_time_between_storms, modeled_TS)\n",
    "\n",
    "rounded_trimmed_discretized_modeled_past = round_to_durations(trimmed_discretized_modeled_past, stormDurations)\n",
    "rounded_trimmed_discretized_modeled_future = round_to_durations(trimmed_discretized_modeled_future, stormDurations)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d062a730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_to_storms(discretized_historical, dry_time_between_storms, TS):\n",
    "    '''Reads in discretized storms and trims out long periods of dry time between them. Returns a table with the duration\n",
    "       of each storm in minutes rounded to the nearest usable length.'''\n",
    "    dry_counter = 0 # intialize counter that counts number of consecutive dry time steps\n",
    "    \n",
    "    for count, precip in enumerate(discretized_historical['pr']):\n",
    "        dry_counter += 1\n",
    "        if precip > 0:\n",
    "            dry_counter = 0\n",
    "\n",
    "    #     print(precip, dry_counter)\n",
    "        if dry_counter >= dry_time_between_storms / hist_TS:\n",
    "            discretized_historical = discretized_historical.drop([count])\n",
    "#         print(precip, dry_counter)\n",
    "\n",
    "\n",
    "    storm_durations = discretized_historical.groupby(['storm_no']).count() * TS\n",
    "    storm_durations['duration'] = storm_durations['datetime']\n",
    "    storm_durations = storm_durations.drop(['datetime', 'pr'], axis = 1)\n",
    "    \n",
    "#     display(storm_durations)\n",
    "    \n",
    "    storm_sums = discretized_historical.groupby(['storm_no'])['pr'].sum()\n",
    "    storm_sums = storm_sums.to_frame()\n",
    "    storm_sums['pr_sum'] = storm_sums['pr']\n",
    "    storm_sums = storm_sums.drop(['pr'], axis = 1)\n",
    "\n",
    "#     display(storm_sums)\n",
    "    \n",
    "    merge_durations_sums = storm_sums.merge(storm_durations, left_on = 'storm_no', right_on = 'storm_no')\n",
    "#     display(merge_durations_sums)\n",
    "\n",
    "#     storm_durations['duration'] = storm_durations['datetime']\n",
    "#     storm_durations = storm_durations.drop(['pr', 'datetime'], axis = 1)\n",
    "    \n",
    "#     display(storm_durations, storm_sums)\n",
    "\n",
    "    return merge_durations_sums\n",
    "\n",
    "# trimmed_discretized_historical = trim_to_storms(discretized_historical, dry_time_between_storms, hist_TS)\n",
    "# trimmed_discretized_historical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cb781f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_to_durations(trimmed_discretized_historical, durations):\n",
    "    '''Rounds the duration of every storm to its closest accepted value.'''\n",
    "    rounded_trimmed_discretized_historical = trimmed_discretized_historical\n",
    "    for count, value in enumerate(rounded_trimmed_discretized_historical['duration']):\n",
    "        if 60 <= value < 90:\n",
    "            value = 60\n",
    "            \n",
    "        elif 90 <= value < 150:\n",
    "            value = 120 \n",
    "            \n",
    "        elif 150 <= value < 270:\n",
    "            value = 180\n",
    "            \n",
    "        elif 270 <= value < 540:\n",
    "            value = 360\n",
    "            \n",
    "        elif 540 <= value < 1080:\n",
    "            value = 720\n",
    "            \n",
    "        else:\n",
    "            value = 1440\n",
    "            \n",
    "        rounded_trimmed_discretized_historical['duration'].iloc[count] = value\n",
    "        \n",
    "    return rounded_trimmed_discretized_historical\n",
    "\n",
    "# rounded_trimmed_discretized_historical = round_to_durations(trimmed_discretized_historical, stormDurations)\n",
    "# rounded_trimmed_discretized_historical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ea1d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>pr</th>\n",
       "      <th>storm_no</th>\n",
       "      <th>pr_sum</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-01-01 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>360.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-01-01 00:05:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>360.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-01-01 00:10:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>360.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-01-01 00:15:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>360.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-01-01 00:20:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>360.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103763</th>\n",
       "      <td>2012-12-27 10:25:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>115</td>\n",
       "      <td>555.96</td>\n",
       "      <td>1440.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103764</th>\n",
       "      <td>2012-12-27 10:30:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>115</td>\n",
       "      <td>555.96</td>\n",
       "      <td>1440.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103765</th>\n",
       "      <td>2012-12-27 10:35:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>115</td>\n",
       "      <td>555.96</td>\n",
       "      <td>1440.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103766</th>\n",
       "      <td>2012-12-27 10:40:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>115</td>\n",
       "      <td>555.96</td>\n",
       "      <td>1440.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103767</th>\n",
       "      <td>2012-12-27 10:45:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>115</td>\n",
       "      <td>555.96</td>\n",
       "      <td>1440.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>103768 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  datetime   pr storm_no  pr_sum  duration\n",
       "0      2012-01-01 00:00:00  0.0        1    0.00     360.0\n",
       "1      2012-01-01 00:05:00  0.0        1    0.00     360.0\n",
       "2      2012-01-01 00:10:00  0.0        1    0.00     360.0\n",
       "3      2012-01-01 00:15:00  0.0        1    0.00     360.0\n",
       "4      2012-01-01 00:20:00  0.0        1    0.00     360.0\n",
       "...                    ...  ...      ...     ...       ...\n",
       "103763 2012-12-27 10:25:00  0.0      115  555.96    1440.0\n",
       "103764 2012-12-27 10:30:00  0.0      115  555.96    1440.0\n",
       "103765 2012-12-27 10:35:00  0.0      115  555.96    1440.0\n",
       "103766 2012-12-27 10:40:00  0.0      115  555.96    1440.0\n",
       "103767 2012-12-27 10:45:00  0.0      115  555.96    1440.0\n",
       "\n",
       "[103768 rows x 5 columns]"
      ]
     },
     "execution_count": 1265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def merge_dfs(discretized_historical, rounded_trimmed_discretized_historical):\n",
    "    merged = discretized_historical.merge(rounded_trimmed_discretized_historical, left_on = 'storm_no', right_on = 'storm_no')\n",
    "\n",
    "    return merged\n",
    "merged_historical = merge_dfs(discretized_historical, rounded_trimmed_discretized_historical,)\n",
    "merged_historical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb5437e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(merged_historical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af136690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate statistics for each storm duration based on full rolling input of ~past~ data\n",
    "past_mean_pr = 0\n",
    "past_std_pr = 0\n",
    "# print(past_std_pr)\n",
    "past_mean_pr = past_max_each_year.groupby(['duration']).mean()\n",
    "# print(copy_past_max_each_year)\n",
    "past_std_pr  = past_max_each_year.groupby(['duration']).std()\n",
    "euler_mascheroni_constant = 0.57721566490153286060651209008240243104215933593992\n",
    "\n",
    "# calculate shape factors from statistics for each duration\n",
    "past_beta = past_std_pr * math.sqrt(6) / math.pi\n",
    "past_mu = past_mean_pr - past_beta * euler_mascheroni_constant\n",
    "\n",
    "# calculate statistics for each storm duration based on full rolling input of ~future~ data\n",
    "future_mean_pr = future_max_each_year.groupby(['duration']).mean()\n",
    "future_std_pr  = future_max_each_year.groupby(['duration']).std()\n",
    "\n",
    "# calculate shape factors from statistics for each duration\n",
    "future_beta = future_std_pr * math.sqrt(6) / math.pi\n",
    "future_mu = future_mean_pr - future_beta * euler_mascheroni_constant\n",
    "\n",
    "# print(copy_past_max_each_year.max())\n",
    "# print(past_mean_pr, past_std_pr, future_mean_pr, future_std_pr)\n",
    "# print(past_beta, past_mu, future_beta, future_mu)\n",
    "\n",
    "def project(past_beta, past_mu, future_beta, future_mu, merged_historical):\n",
    "    \n",
    "#     # remove zero values from each TS  \n",
    "#     past_rain_only = past_storms_data[past_storms_data['pr'] != 0]\n",
    "#     future_rain_only = future_storms_data[future_storms_data['pr'] != 0]\n",
    "#     future_rain_only['pr'] = future_rain_only['pr'] * 1.2\n",
    "        \n",
    "#     # calculate statistics for each storm duration based on full rolling input of ~past~ data\n",
    "#     past_mean_pr = past_rain_only['pr'].mean()\n",
    "#     past_std_pr  = past_rain_only['pr'].std()\n",
    "#     euler_mascheroni_constant = 0.57721566490153286060651209008240243104215933593992\n",
    "\n",
    "    \n",
    "#     print(past_mean_pr, past_std_pr)\n",
    "#     print(np.exp(-1 * np.exp(-1 * (value / merged_historical.loc[count, 'duration']) - float(past_mu.loc[merged_historical['duration'].iloc[count]])))/ float(past_beta.loc[merged_historical['duration'].iloc[count]]))\n",
    "\n",
    "    for count, value in enumerate(merged_historical['pr']):\n",
    "# #         print(past_beta.loc[past_max_each_year['duration'].iloc[count]])\n",
    "# #         print(past_mu.loc[merged_historical['duration'].iloc[count]])\n",
    "#         print(math.exp(-1 * math.exp(-1 * (value - float(past_mu.loc[merged_historical['duration'].iloc[count]])) / float(past_beta.loc[merged_historical['duration'].iloc[count]]))))\n",
    "        merged_historical.loc[count, 'CDF'] = math.exp(-1 * math.exp(-1 * (value - float(past_mu.loc[merged_historical['duration'].iloc[count]])) / float(past_beta.loc[merged_historical['duration'].iloc[count]])))\n",
    "#         merged_historical.loc[count, 'projected_pr_sum'] = float(-1 * float(future_beta.loc[merged_historical['duration'].iloc[count]]) * (np.log(-1 * np.log(merged_historical.loc[count, 'CDF']))) + float(future_mu.loc[merged_historical['duration'].iloc[count]]))\n",
    "        merged_historical.loc[count, 'projected_pr_sum'] = -1 * float(future_beta.loc[merged_historical['duration'].iloc[count]]) * (math.log(-1 * math.log(float(merged_historical.loc[count, 'CDF'])))) + float(future_mu.loc[merged_historical['duration'].iloc[count]])\n",
    "#         merged_historical.loc[count, 'delta'] = merged_historical.projected_pr_sum[count] / projected.pr_sum[count]\n",
    "#         merged_historical.loc[count, 'projected_pr'] = merged_historical.delta * merged_historical.pr\n",
    "\n",
    "        \n",
    "#         merged_historical.loc[count, 'delta'] = merged_historical.loc[count, 'projected_pr'] / merged_historical.loc[count, 'pr_sum']\n",
    "#         merged_historical.loc[count, 'Projected pr'] = merged_historical.loc[count, 'pr'] * merged_historical.loc[count, 'delta']\n",
    "\n",
    "# merged_historical.loc[count, 'delta'] = math.exp(-1 * math.exp(-1 * value - past_mu.loc[merged_historical['duration'].iloc[count]])) / past_beta.loc[merged_historical['duration'].iloc[count]]\n",
    "#         merged_historical.loc[count, 'CDF'] = float(math.exp(-1 * math.exp(-1 * (value - past_mu.loc[past_max_each_year['duration'].iloc[count]]) / past_beta.loc[past_max_each_year['duration'].iloc[count]])))\n",
    "#         merged_historical.loc[count, 'projected_pr'] = float(-1 * future_beta.loc[past_max_each_year['duration'].iloc[count]] * (math.log(-1 * math.log(past_max_each_year.loc[count, 'CDF']))) + future_mu.loc[past_max_each_year['duration'].iloc[count]])\n",
    "#         print(past_max_each_year.loc[count, 'CDF'])\n",
    "#         past_max_each_year.loc[count, 'projected_pr'] = future_beta.loc[past_max_each_year['duration'].iloc[count]]\n",
    "#         print(-1 * future_beta.loc[past_max_each_year['duration'].iloc[count]] * (math.log(-1 * math.log(past_max_each_year.loc[count, 'CDF']))) + future_mu.loc[past_max_each_year['duration'].iloc[count]])\n",
    "#         print(future_beta.loc[past_max_each_year['duration'].iloc[count]], future_mu.loc[past_max_each_year['duration'].iloc[count]], past_max_each_year.loc[count, 'CDF'])\n",
    "    \n",
    "#         past_max_each_year.loc[count, 'delta'] = past_max_each_year.loc[count, 'projected_pr'] / past_max_each_year.loc[count, 'pr_sum']    \n",
    "    return(merged_historical)\n",
    "projected = 0\n",
    "projected = project(past_beta, past_mu, future_beta, future_mu, merged_historical)\n",
    "projected\n",
    "# float(-1 * future_beta.loc[past_max_each_year['duration'].iloc[count]] * (math.log(-1 * math.log(past_max_each_year.loc[count, 'CDF']))) + future_mu.loc[past_max_each_year['duration'].iloc[count]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c261f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate statistics for each storm duration based on full rolling input of ~past~ data\n",
    "past_mean_pr = 0\n",
    "past_std_pr = 0\n",
    "# print(past_std_pr)\n",
    "past_mean_pr = past_max_each_year.groupby(['duration']).mean()\n",
    "# print(copy_past_max_each_year)\n",
    "past_std_pr  = past_max_each_year.groupby(['duration']).std()\n",
    "euler_mascheroni_constant = 0.57721566490153286060651209008240243104215933593992\n",
    "\n",
    "# calculate shape factors from statistics for each duration\n",
    "past_beta = past_std_pr * math.sqrt(6) / math.pi\n",
    "past_mu = past_mean_pr - past_beta * euler_mascheroni_constant\n",
    "\n",
    "# calculate statistics for each storm duration based on full rolling input of ~future~ data\n",
    "future_mean_pr = future_max_each_year.groupby(['duration']).mean()\n",
    "future_std_pr  = future_max_each_year.groupby(['duration']).std()\n",
    "\n",
    "# calculate shape factors from statistics for each duration\n",
    "future_beta = future_std_pr * math.sqrt(6) / math.pi\n",
    "future_mu = future_mean_pr - future_beta * euler_mascheroni_constant\n",
    "\n",
    "# print(copy_past_max_each_year.max())\n",
    "# print(past_mean_pr, past_std_pr, future_mean_pr, future_std_pr)\n",
    "# print(past_beta, past_mu, future_beta, future_mu)\n",
    "\n",
    "def project(past_beta, past_mu, future_beta, future_mu, merged_historical):\n",
    "    \n",
    "#     # remove zero values from each TS  \n",
    "#     past_rain_only = past_storms_data[past_storms_data['pr'] != 0]\n",
    "#     future_rain_only = future_storms_data[future_storms_data['pr'] != 0]\n",
    "#     future_rain_only['pr'] = future_rain_only['pr'] * 1.2\n",
    "        \n",
    "#     # calculate statistics for each storm duration based on full rolling input of ~past~ data\n",
    "#     past_mean_pr = past_rain_only['pr'].mean()\n",
    "#     past_std_pr  = past_rain_only['pr'].std()\n",
    "#     euler_mascheroni_constant = 0.57721566490153286060651209008240243104215933593992\n",
    "\n",
    "    \n",
    "#     print(past_mean_pr, past_std_pr)\n",
    "#     print(np.exp(-1 * np.exp(-1 * (value / merged_historical.loc[count, 'duration']) - float(past_mu.loc[merged_historical['duration'].iloc[count]])))/ float(past_beta.loc[merged_historical['duration'].iloc[count]]))\n",
    "\n",
    "    for count, value in enumerate(merged_historical.duration):\n",
    "# #         print(past_beta.loc[past_max_each_year['duration'].iloc[count]])\n",
    "# #         print(past_mu.loc[merged_historical['duration'].iloc[count]])\n",
    "#         print(math.exp(-1 * math.exp(-1 * (value - float(past_mu.loc[merged_historical['duration'].iloc[count]])) / float(past_beta.loc[merged_historical['duration'].iloc[count]]))))\n",
    "        merged_historical.loc[count, 'CDF'] = math.exp(-1 * math.exp(-1 * (value - float(past_mu.loc[merged_historical['duration'].iloc[count]])) / float(past_beta.loc[merged_historical['duration'].iloc[count]])))\n",
    "#         merged_historical.loc[count, 'projected_pr_sum'] = float(-1 * float(future_beta.loc[merged_historical['duration'].iloc[count]]) * (np.log(-1 * np.log(merged_historical.loc[count, 'CDF']))) + float(future_mu.loc[merged_historical['duration'].iloc[count]]))\n",
    "        merged_historical.loc[count, 'projected_pr'] = -1 * float(future_beta.loc[merged_historical['duration'].iloc[count]]) * (math.log(-1 * math.log(float(merged_historical.loc[count, 'CDF'])))) + float(future_mu.loc[merged_historical['duration'].iloc[count]])\n",
    "\n",
    "#         merged_historical.loc[count, 'delta'] = merged_historical.loc[count, 'projected_pr'] / merged_historical.loc[count, 'pr_sum']\n",
    "#         merged_historical.loc[count, 'Projected pr'] = merged_historical.loc[count, 'pr'] * merged_historical.loc[count, 'delta']\n",
    "\n",
    "# merged_historical.loc[count, 'delta'] = math.exp(-1 * math.exp(-1 * value - past_mu.loc[merged_historical['duration'].iloc[count]])) / past_beta.loc[merged_historical['duration'].iloc[count]]\n",
    "#         merged_historical.loc[count, 'CDF'] = float(math.exp(-1 * math.exp(-1 * (value - past_mu.loc[past_max_each_year['duration'].iloc[count]]) / past_beta.loc[past_max_each_year['duration'].iloc[count]])))\n",
    "#         merged_historical.loc[count, 'projected_pr'] = float(-1 * future_beta.loc[past_max_each_year['duration'].iloc[count]] * (math.log(-1 * math.log(past_max_each_year.loc[count, 'CDF']))) + future_mu.loc[past_max_each_year['duration'].iloc[count]])\n",
    "#         print(past_max_each_year.loc[count, 'CDF'])\n",
    "#         past_max_each_year.loc[count, 'projected_pr'] = future_beta.loc[past_max_each_year['duration'].iloc[count]]\n",
    "#         print(-1 * future_beta.loc[past_max_each_year['duration'].iloc[count]] * (math.log(-1 * math.log(past_max_each_year.loc[count, 'CDF']))) + future_mu.loc[past_max_each_year['duration'].iloc[count]])\n",
    "#         print(future_beta.loc[past_max_each_year['duration'].iloc[count]], future_mu.loc[past_max_each_year['duration'].iloc[count]], past_max_each_year.loc[count, 'CDF'])\n",
    "    \n",
    "#         past_max_each_year.loc[count, 'delta'] = past_max_each_year.loc[count, 'projected_pr'] / past_max_each_year.loc[count, 'pr_sum']    \n",
    "    return(merged_historical)\n",
    "projected = 0\n",
    "projected = project(past_beta, past_mu, future_beta, future_mu, merged_historical)\n",
    "projected\n",
    "# float(-1 * future_beta.loc[past_max_each_year['duration'].iloc[count]] * (math.log(-1 * math.log(past_max_each_year.loc[count, 'CDF']))) + future_mu.loc[past_max_each_year['duration'].iloc[count]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876b8461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project(past_beta, past_mu, future_beta, future_mu, merged_historical):\n",
    "\n",
    "    past_beta = past_beta.loc[360]\n",
    "    past_mu = past_mu.loc[360]\n",
    "    future_beta = future_beta.loc[360]\n",
    "    future_mu = future_mu.loc[360]\n",
    "    \n",
    "    for count, value in enumerate(merged_historical[merged_historical.duration == 360].index):\n",
    "#         merged_historical.loc[value, 'CDF'] = math.exp(-1 * math.exp(-1 * (merged_historical.pr[value] - past_mu) / past_beta))\n",
    "#         merged_historical.loc[value, 'projected_pr'] = -1 * future_beta * (math.log(-1 * math.log(merged_historical.loc[value, 'CDF']))) + future_mu\n",
    "        merged_historical.loc[value, 'projected_pr'] = value #-1 * future_beta * (math.log(-1 * math.log(merged_historical.loc[value, 'CDF']))) + future_mu\n",
    "\n",
    "\n",
    "\n",
    "#         merged_historical.loc[count, 'CDF'] = math.exp(-1 * math.exp(-1 * (value - past_mu) / past_beta))\n",
    "#         merged_historical.loc[count, 'projected_pr'] = -1 * future_beta * (math.log(-1 * math.log(merged_historical.loc[count, 'CDF']))) + future_mu\n",
    "\n",
    "    return(merged_historical)\n",
    "projected = 0\n",
    "projected = project(past_beta, past_mu, future_beta, future_mu, merged_historical)\n",
    "projected\n",
    "# float(-1 * future_beta.loc[past_max_each_year['duration'].iloc[count]] * (math.log(-1 * math.log(past_max_each_year.loc[count, 'CDF']))) + future_mu.loc[past_max_each_year['duration'].iloc[count]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e27d1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(projected.projected_pr_sum.max())\n",
    "projected.projected_pr_sum = abs(projected.projected_pr_sum)\n",
    "print([i for i in projected.projected_pr_sum])\n",
    "\n",
    "# merged_historical['projected_pr_sum']\n",
    "# merged_historical['delta'] = merged_historical.projected_pr_sum / merged_historical.pr_sum\n",
    "# merged_historical['projected_pr'] = merged_historical.pr * merged_historical.delta\n",
    "\n",
    "# merged_historical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b87de79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#         merged_historical.loc[count, 'Projected pr'] = merged_historical.loc[count, 'pr'] * merged_historical.loc[count, 'delta']\n",
    "projected['delta'] = projected.projected_pr_sum / projected.pr_sum\n",
    "projected['projected_pr'] = projected.delta * projected.pr\n",
    "\n",
    "# print(projected.pr * projected.delta)\n",
    "# for count, i in enumerate(projected.pr):\n",
    "#     projected.iloc[count, 'projected_pr'] = i * projected.delta[count])\n",
    "\n",
    "# projected['projected_pr'] = projected.pr * projected.delta\n",
    "# projected.loc['NaN'] = 0\n",
    "projected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0553b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "projected.iloc[640:670] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5270d40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_by_intensity(past_storms_data, future_storms_data, historical):\n",
    "    \n",
    "    # remove zero values from each TS  \n",
    "    past_rain_only = past_storms_data[past_storms_data['pr'] != 0]\n",
    "    future_rain_only = future_storms_data[future_storms_data['pr'] != 0]\n",
    "#     future_rain_only['pr'] = future_rain_only['pr'] * 1.2\n",
    "        \n",
    "    # calculate statistics for each storm duration based on full rolling input of ~past~ data\n",
    "    past_mean_pr = past_rain_only['pr'].mean()\n",
    "    past_std_pr  = past_rain_only['pr'].std()\n",
    "    euler_mascheroni_constant = 0.57721566490153286060651209008240243104215933593992\n",
    "    \n",
    "    # calculate shape factors from statistics for each duration\n",
    "    past_beta = past_std_pr * math.sqrt(6) / math.pi\n",
    "    past_mu = past_mean_pr - past_beta * euler_mascheroni_constant\n",
    "\n",
    "#     print(past_mean_pr)\n",
    "#     print(past_std_pr)\n",
    "    # print(past_beta)\n",
    "\n",
    "    # calculate statistics for each storm duration based on full rolling input of ~future~ data\n",
    "    future_mean_pr = future_rain_only['pr'].mean()\n",
    "    future_std_pr  = future_rain_only['pr'].std()\n",
    "#     print(future_mean_pr, future_std_pr)\n",
    "\n",
    "    # calculate shape factors from statistics for each duration\n",
    "    future_beta = future_std_pr * math.sqrt(6) / math.pi\n",
    "    future_mu = future_mean_pr - future_beta * euler_mascheroni_constant\n",
    "    \n",
    "#     print(past_beta, past_mu, future_beta, future_mu)\n",
    "#     print(past_mu.pr)\n",
    "#     display(historical)\n",
    "#     print(np.exp(-1 * np.exp(-1 *(14 - past_mu) / past_beta)))\n",
    "#     print(-1 * future_beta * (math.log(-1 * math.log(.1))) + future_mu)\n",
    "    for count, value in enumerate(historical.loc[ : ,'pr']):\n",
    "        historical.loc[count, 'CDF'] = math.exp(-1 * math.exp(-1 * (value - float(past_mu)) / float(past_beta)))\n",
    "        historical.loc[count, 'Projected pr'] = -1 * future_beta * (math.log(-1 * math.log(historical.loc[count, 'CDF']))) + future_mu\n",
    "#         historical.loc[count, 'Delta'] = historical.loc[count, 'Projected pr'] / historical.loc[count, 'pr']\n",
    "\n",
    "\n",
    "    \n",
    "    return historical\n",
    "projected_by_intensity = project_by_intensity(past_storms_data, future_storms_data, historical)\n",
    "display(projected_by_intensity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68c9d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = historical['datetime']\n",
    "y1 = historical['pr']\n",
    "\n",
    "x2 = x1\n",
    "y2 = projected_by_intensity['Projected pr']\n",
    "\n",
    "x3 = projected.datetime\n",
    "y3 = projected.projected_pr\n",
    "\n",
    "plt.rcParams.update({'font.size': 24})\n",
    "\n",
    "# Create the line plot with different colors\n",
    "plt.figure(figsize=(18,15))\n",
    "plt.plot(x2, y2, label='Individual Intensity Projection', color = 'orange')\n",
    "plt.plot(x1, y1, label='Historical (Observed) Precipitation')\n",
    "plt.plot(x3, y3, label='Storm-Storm Projection', color = 'green')\n",
    "\n",
    "# plt.xlim(15641, 15643.5)  # Set x-axis limits from 0 to 20\n",
    "# plt.ylim(-.25, 7)   # Set y-axis limits from -2 to 20 \n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Date\", fontsize = 32)\n",
    "plt.ylabel(\"Precipitation (mm/hr)\", fontsize = 32)\n",
    "# plt.title(\"Coarse Time Step\", fontsize = 36)\n",
    "\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "order = [1,0,2]\n",
    "plt.legend([handles[idx] for idx in order],[labels[idx] for idx in order])\n",
    "\n",
    "# Add legend\n",
    "# plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf37212",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = historical['datetime']\n",
    "y1 = historical['pr']\n",
    "\n",
    "x2 = x1\n",
    "y2 = projected_by_intensity['Projected pr']\n",
    "\n",
    "x3 = projected.datetime\n",
    "y3 = projected.projected_pr\n",
    "\n",
    "plt.rcParams.update({'font.size': 24})\n",
    "\n",
    "# Create the line plot with different colors\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(x2, y2, label='Intensity Projection')\n",
    "plt.plot(x1, y1, label='Historical (Observed) Precipitation')\n",
    "plt.plot(x3, y3, label='Storm-Storm Projection')\n",
    "\n",
    "plt.xlim(15641, 15643.5)  # Set x-axis limits from 0 to 20\n",
    "# plt.ylim(-.25, 7)   # Set y-axis limits from -2 to 20 \n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Date\", fontsize = 32)\n",
    "plt.ylabel(\"Precipitation (mm/hr)\", fontsize = 32)\n",
    "# plt.title(\"Coarse Time Step\", fontsize = 36)\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4549f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(projected.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a260e237",
   "metadata": {},
   "outputs": [],
   "source": [
    "storms_data = read_csv('sample_past_precip.csv')\n",
    "storms_data = read_netcdf4('CONUS404-19791001-20221001-Dover-Air-Force-Base.nc')\n",
    "TS = find_timestep(storms_data)\n",
    "max_each_year = separate_into_durations(storms_data, stormDurations)\n",
    "past_CDF = compute_CDF(max_each_year, stormDurations)\n",
    "historical = read_historical_csv('2012_historical.csv')\n",
    "hist_TS = find_timestep(historical)\n",
    "discretize_by_storm(historical, dry_time_between_storms)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
