{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import packaging\n",
    "import matplotlib.pyplot as plt\n",
    "import netCDF4\n",
    "import math\n",
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "# Define functions\n",
    "def read_netcdf4(filename):\n",
    "    '''Read in data from netcdf4, assign headers, set datatypes, perform unit conversions if necessary'''\n",
    "    # read in netcdf4 data to xarray\n",
    "    storms_data = xr.open_dataset(filename)\n",
    "    \n",
    "    # convert xarray dataarray to pandas dataframe\n",
    "    storms_data = storms_data.RAINRATE.to_dataframe()\n",
    "    \n",
    "    # tidy data\n",
    "    storms_data = storms_data.drop(['lat', 'lon'], axis=1)\n",
    "    storms_data = storms_data.droplevel(['x', 'y'])\n",
    "    storms_data['datetime'] = storms_data.index\n",
    "    storms_data = storms_data.reset_index(drop = True)\n",
    "    storms_data['datetime']= pd.to_datetime(storms_data['datetime'])\n",
    "    storms_data = storms_data.rename(columns={\"RAINRATE\": \"pr\"})\n",
    "    storms_data = storms_data.iloc[:,[1, 0]]\n",
    "    # perform unit conversions if necessary\n",
    "    # default units for netcdf4 are mm to 0.1 mm accuracy\n",
    "    # storms_data.loc[ : , 'pr'] = storms_data.loc[ : , 'pr']\n",
    "\n",
    "    return storms_data\n",
    "\n",
    "def read_csv(filename):\n",
    "    '''Read in data from csv, assign headers, set datatypes, perform unit conversions if necessary, and find time step for\n",
    "       the dataset in minutes.'''\n",
    "# read data from csv\n",
    "    headers = ['datetime', 'pr']\n",
    "    dtypes = {'col1': 'str', 'col2': 'float'}\n",
    "    date_cols = ['datetime']\n",
    "    storms_data = pd.read_csv(filename, dtype = dtypes, header=None, names = headers, skiprows = [0, 1], parse_dates = date_cols)\n",
    "    \n",
    "    # storms_data = pd.concat([storms_data, storms_new], ignore_index = True)\n",
    "    \n",
    "    # perform unit conversions\n",
    "    storms_data.loc[ : , 'pr'] = storms_data.loc[ : , 'pr']\n",
    "        \n",
    "    return storms_data\n",
    "\n",
    "def find_timestep(storms_data):\n",
    "    '''Find time step of dataset in minutes by subtracting two datetimes'''\n",
    "    # determine time step by subtracting one datetime from another\n",
    "    TS = storms_data.iloc[1, 0] - storms_data.iloc[0, 0]\n",
    "    # convert timedelta type to int type in minutes\n",
    "    TS = TS.total_seconds() / 60\n",
    "    return TS\n",
    "\n",
    "def project_by_intensity(storms_modeled_past, storms_modeled_future, storms_obs, TS, subset):\n",
    "    '''This is the meat of the process. It accepts model and historical data, finds Gumbel shape factors from each dataset, uses them to find\n",
    "    the CDF value that corresponds with each precipitation intensity, calculates delta factors, and returns a time series of historical\n",
    "    precipitation that has been projected into the future.'''\n",
    "    \n",
    "    # resample observed dataset to find hourly sums. This aligns the volumes from the hourly model data and the 5-min observed data\n",
    "    resample = storms_obs.resample('60min', on = 'datetime').sum()\n",
    "    # adjust index to include same number of entries as storms_obs\n",
    "    resample = resample.loc[resample.index.repeat(60 / TS)].reset_index()\n",
    "    # add resampled data to storms_obs\n",
    "    storms_obs['pr_hourly'] = resample.pr\n",
    "    \n",
    "    # remove all dry times from datasets - this way only rainy periods are used to characterize the precipitation\n",
    "    storms_modeled_past_no_0 = storms_modeled_past[storms_modeled_past['pr'] != 0]\n",
    "    storms_modeled_future_no_0 = storms_modeled_future[storms_modeled_future['pr'] != 0]\n",
    "    storms_obs_no_0 = storms_obs[storms_obs['pr_hourly'] != 0]\n",
    "    \n",
    "            \n",
    "    # calculate statistics for each storm duration based on full rolling input of ~past~ data and find shape factors\n",
    "    past_mean_pr = storms_modeled_past_no_0['pr'].mean()\n",
    "    past_std_pr  = storms_modeled_past_no_0['pr'].std()\n",
    "    euler_mascheroni_constant = 0.577215\n",
    "    past_beta = past_std_pr * math.sqrt(6) / math.pi\n",
    "    past_mu = past_mean_pr - past_beta * euler_mascheroni_constant\n",
    "\n",
    "    # calculate statistics for each storm duration based on full rolling input of ~future~ data and find shape factors\n",
    "    future_mean_pr = storms_modeled_future_no_0['pr'].mean()\n",
    "    future_std_pr  = storms_modeled_future_no_0['pr'].std()\n",
    "    future_beta = future_std_pr * math.sqrt(6) / math.pi\n",
    "    future_mu = future_mean_pr - future_beta * euler_mascheroni_constant\n",
    "        \n",
    "    # calculate statistics for each storm duration based on full rolling input of ~observed~ data and find shape factors\n",
    "    obs_mean_pr = storms_obs_no_0['pr_hourly'].mean()\n",
    "    obs_std_pr  = storms_obs_no_0['pr_hourly'].std()\n",
    "    obs_beta = obs_std_pr * math.sqrt(6) / math.pi\n",
    "    obs_mu = obs_mean_pr - obs_beta * euler_mascheroni_constant\n",
    "    \n",
    "    \n",
    "    # find the associated CDF value for each precipitation time step, solve for delta factors, and project the observed values into the future\n",
    "    for count, value in enumerate(subset.loc[ : ,'pr']):\n",
    "        subset.loc[count, 'obs_CDF'] = math.exp(-1 * math.exp(-1 * (value - float(obs_mu)) / float(obs_beta)))\n",
    "        subset.loc[count, 'past_CDF'] = math.exp(-1 * math.exp(-1 * (value - float(past_mu)) / float(past_beta)))\n",
    "        subset.loc[count, 'future_CDF'] = math.exp(-1 * math.exp(-1 * (value - float(future_mu)) / float(future_beta)))\n",
    "        subset.loc[count, 'past_pr'] = -1 * past_beta * (math.log(-1 * math.log(subset.loc[count, 'obs_CDF']))) + past_mu\n",
    "        subset.loc[count, 'future_pr'] = -1 * future_beta * (math.log(-1 * math.log(subset.loc[count, 'obs_CDF']))) + future_mu\n",
    "        subset.loc[count, 'delta'] = subset.loc[count, 'future_pr'] / subset.loc[count, 'past_pr']\n",
    "        subset.loc[count, 'projected_pr'] = subset.loc[count, 'delta'] * value\n",
    "        \n",
    "        \n",
    "    # Add switch to fix edge case where model CDFs are negative for the smallest precipitation values\n",
    "    # this is caused by the model under predicting precipitation for the study year\n",
    "    subset.delta[subset.past_pr < 0] = 1\n",
    "    subset.delta[subset.future_pr < 0] = 1\n",
    "    subset.projected_pr[subset.delta == 1] = subset.pr\n",
    "\n",
    "    return subset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read netCDF4 data - climate model results\n",
    "modeled_data = read_netcdf4('CONUS404-19791001-20221001-Dover-Air-Force-Base.nc')\n",
    "\n",
    "# The following is used only while I don't have the full future CONUS404 dataset. It splits the past data in half \n",
    "# and pretends that the latter part is from the future\n",
    "storms_modeled_past = modeled_data.iloc[:188471,:]\n",
    "storms_modeled_future = modeled_data.iloc[188471:,:]\n",
    "storms_modeled_future.reset_index(inplace = True, drop = True)\n",
    "\n",
    "# Read full extent of observed data - tipping bucket rain gauge\n",
    "# Names of all data files\n",
    "files = ['files/dover_rain_gauge/DDFS.01-JAN-2005=01-JAN-2010.csv', \n",
    "         'files/dover_rain_gauge/DDFS.01-JAN-2010=01-JAN-2015.csv',\n",
    "         'files/dover_rain_gauge/DDFS.01-JAN-2015=01-JAN-2020.csv',\n",
    "         'files/dover_rain_gauge/DDFS.01-JAN-2020=01-NOV-2023.csv'\n",
    "        ]\n",
    "\n",
    "# Read each file individually\n",
    "storms_obs0 = read_csv(files[0])\n",
    "storms_obs1 = read_csv(files[1])\n",
    "storms_obs2 = read_csv(files[2])\n",
    "storms_obs3 = read_csv(files[3])\n",
    "\n",
    "# Combine files into one dataframe\n",
    "storms_obs = pd.concat([storms_obs0, storms_obs1, storms_obs2, storms_obs3], ignore_index = True)\n",
    "\n",
    "# Replace all negative/error values with 0\n",
    "# The data from this site has some error values because the tipping bucket rain gauge doesn't work well with snow\n",
    "storms_obs.pr[storms_obs.pr < 0] = 0\n",
    "\n",
    "# Read a time series which is to be projected into the future\n",
    "data_2012 = read_csv('2012_historical.csv')\n",
    "\n",
    "# Find the timestep of the observed data\n",
    "TS = find_timestep(storms_obs)\n",
    "\n",
    "# Project yime series and display results\n",
    "projected_by_intensity = 0\n",
    "projected_by_intensity = project_by_intensity(storms_modeled_past, storms_modeled_future, storms_obs, TS, data_2012)\n",
    "display(projected_by_intensity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
